{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446fb61f-8adb-4fd2-a1c3-e331904e4674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News DataFrame after preprocessing:\n",
      "                                                text  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...   \n",
      "1  House Intelligence Committee Chairman Devin Nu...   \n",
      "2  On Friday, it was revealed that former Milwauk...   \n",
      "3  On Christmas day, Donald Trump announced that ...   \n",
      "4  Pope Francis used his annual Christmas Day mes...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  donald trump wish american happy new year leav...  \n",
      "1  house intelligence committee chairman devin nu...  \n",
      "2  friday revealed former milwaukee sheriff david...  \n",
      "3  christmas day donald trump announced would bac...  \n",
      "4  pope francis used annual christmas day message...  \n",
      "\n",
      "Real News DataFrame after preprocessing:\n",
      "                                                text  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  washington reuters head conservative republica...  \n",
      "1  washington reuters transgender people allowed ...  \n",
      "2  washington reuters special counsel investigati...  \n",
      "3  washington reuters trump campaign adviser geor...  \n",
      "4  reuters president donald trump called postal s...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources (uncomment if not downloaded)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Define the folder path containing the CSV files\n",
    "folder_path = '/Users/aamershah/Desktop/gig/News_dataset'\n",
    "\n",
    "# Define the file names\n",
    "fake_file = 'Updated_Fake.csv'\n",
    "real_file = 'Updated_Real.csv'\n",
    "\n",
    "# Construct the full paths\n",
    "fake_file_path = os.path.join(folder_path, fake_file)\n",
    "real_file_path = os.path.join(folder_path, real_file)\n",
    "\n",
    "# Load the updated fake and real news data\n",
    "fake_df = pd.read_csv(fake_file_path)\n",
    "real_df = pd.read_csv(real_file_path)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing to the 'text' column in both DataFrames\n",
    "fake_df['preprocessed_text'] = fake_df['text'].apply(preprocess_text)\n",
    "real_df['preprocessed_text'] = real_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the updated DataFrames to verify preprocessing\n",
    "print(\"Fake News DataFrame after preprocessing:\")\n",
    "print(fake_df[['text', 'preprocessed_text']].head())\n",
    "\n",
    "print(\"\\nReal News DataFrame after preprocessing:\")\n",
    "print(real_df[['text', 'preprocessed_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b8db08-a911-4cc2-9765-3ce5703ab4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Fake News Subjects:\n",
      "subject\n",
      "politics     16640\n",
      "worldnews    16640\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced Real News Subjects:\n",
      "subject\n",
      "politics     16640\n",
      "worldnews    16640\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of occurrences for each subject in both datasets\n",
    "subject_counts_fake = fake_df['subject'].value_counts()\n",
    "subject_counts_real = real_df['subject'].value_counts()\n",
    "\n",
    "# Find the maximum count for subjects in both datasets to determine the resampling target\n",
    "resample_size = max(subject_counts_fake.max(), subject_counts_real.max())\n",
    "\n",
    "# Function to resample each subject to the target size\n",
    "def resample_subjects(df, target_size):\n",
    "    # Use 'replace=True' to allow oversampling by sampling with replacement\n",
    "    return df.groupby('subject').apply(lambda x: x.sample(target_size, replace=True)).reset_index(drop=True)\n",
    "\n",
    "# Resample both DataFrames\n",
    "balanced_fake_df = resample_subjects(fake_df, resample_size)\n",
    "balanced_real_df = resample_subjects(real_df, resample_size)\n",
    "\n",
    "# Verify the balancing by counting the occurrences of each subject in the balanced datasets\n",
    "print(\"Balanced Fake News Subjects:\")\n",
    "print(balanced_fake_df['subject'].value_counts())\n",
    "\n",
    "print(\"\\nBalanced Real News Subjects:\")\n",
    "print(balanced_real_df['subject'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9098a482-83d4-497a-b1c5-c606ab07fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features shape: (66560, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "# Combine the balanced fake and real datasets\n",
    "combined_balanced_df = pd.concat([balanced_fake_df, balanced_real_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataframe to ensure a good mix\n",
    "combined_balanced_df = combined_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Extract features and target\n",
    "X = combined_balanced_df['text']  # the preprocessed text\n",
    "y = combined_balanced_df['label']  # the labels\n",
    "\n",
    "# Fit and transform the text data to create TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(\"TF-IDF features shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cf3a67-44a9-4c60-99e1-6ba16d5c294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       1.00      1.00      1.00      6671\n",
      "        real       1.00      1.00      1.00      6641\n",
      "\n",
      "    accuracy                           1.00     13312\n",
      "   macro avg       1.00      1.00      1.00     13312\n",
      "weighted avg       1.00      1.00      1.00     13312\n",
      "\n",
      "Naive Bayes Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.95      0.92      0.93      6671\n",
      "        real       0.92      0.95      0.94      6641\n",
      "\n",
      "    accuracy                           0.94     13312\n",
      "   macro avg       0.94      0.94      0.94     13312\n",
      "weighted avg       0.94      0.94      0.94     13312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Assuming X_tfidf and y are already defined and represent your feature matrix and labels respectively\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Set up parameter grid to search\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Set up GridSearchCV with SVM\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best estimator found by grid search\n",
    "best_svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set using the best estimator\n",
    "y_pred_svm = best_svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "print(\"SVM Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the Naive Bayes model\n",
    "print(\"Naive Bayes Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39118aa-b217-4aba-9318-817674fe0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define SVM classifier\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Set up parameter grid to search\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best estimator found by grid search\n",
    "best_svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set using the best estimator\n",
    "y_pred_svm = best_svm_classifier.predict(X_test)\n",
    "print(\"SVM Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6256b8b-5a2e-4b97-851a-fcd8fbe5bdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
